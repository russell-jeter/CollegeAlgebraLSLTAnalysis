{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import statistics # Used for statistics.fmean()\n",
    "import os # Used to define file location at end of ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_excel(\"../question_database_schema.xlsx\", sheet_name=\"student_question_responses\")\n",
    "key_df = pd.read_excel(\"../question_database_schema.xlsx\", sheet_name=\"answer_choices\")\n",
    "key_df = key_df[key_df['is_distractor'] == 0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create single exam/version df for Rasch modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions to form 0/1 dataframe with each row being a student and each column being a single exam/version question\n",
    "\n",
    "# Returns dataframe df with just rows with 'question_id' == num_and_ver\n",
    "def exam_num_ver_df(num_and_ver, df):\n",
    "    exam_mask = df.apply(lambda x: x['question_id'].startswith(num_and_ver), axis=1) # Applies lambda function searching for rows (axis=1) with 'question_id' = num_and_ver\n",
    "    exam_df = df[exam_mask]\n",
    "    return exam_df\n",
    "\n",
    "# Reorganizes student responses to a single exam into columns by question_id\n",
    "def questions_to_columns(df):\n",
    "    list_of_unique_id_dicts=[] # initalize empty list for future df\n",
    "    for unique_id in df['student_id'].unique(): # for each student \n",
    "        unique_id_df = df[df['student_id'] == unique_id] # reduce df to just the student responses on the exam\n",
    "        temp_dict={'id': unique_id} # initalize future row as a dict by student id\n",
    "\n",
    "        question_list = unique_id_df['question_id'].tolist() # list of question_ids for columns\n",
    "        selection_list = unique_id_df['selected_option'].tolist() # list of student responses by question_id\n",
    "        \n",
    "        for index in range(0, len(question_list)):\n",
    "            temp_dict[question_list[index]]=selection_list[index] # assign student response to each question_id for future row as a dict\n",
    "        \n",
    "        list_of_unique_id_dicts.append(temp_dict) # add future row as dict to list for future df\n",
    "\n",
    "    restructured_df=pd.DataFrame(list_of_unique_id_dicts) # create dataframe based on list of dicts as rows\n",
    "    restructured_df=restructured_df.set_index('id', drop=True) # set the student ids as the index and drop the old index\n",
    "    bad_question=['4A01', '4B01', '4C01'] # Exam 4 question 1 is \"what version exam do you have\" and is omitted from analysis\n",
    "    for bad_q_label in bad_question: # Removes bad questions from df\n",
    "        if bad_q_label in restructured_df.keys():\n",
    "            restructured_df.drop(bad_q_label, axis=1, inplace=True)\n",
    "    return restructured_df\n",
    "\n",
    "# Creates a key for the num_and_ver exam to be used for the correct/incorrect matrix\n",
    "def create_num_ver_key_dict(num_and_ver, key_df):\n",
    "    exam_key_df = exam_num_ver_df(num_and_ver, key_df) # Takes full key_df and reduces to key_df for just this num_and_ver\n",
    "    question_ids = exam_key_df['question_id'].tolist() # Creates list of question_ids \n",
    "\n",
    "    answer_series=exam_key_df['option_id'].replace({'A': '1', 'B': '2', 'C': '3', 'D': '4', 'E': '5'}) # avoids downcast warning\n",
    "    answer_series=answer_series.astype(int) # manually forces downcasting\n",
    "    answers = answer_series.tolist() # list of answers\n",
    "\n",
    "    key_dict = {} # initalize key dict\n",
    "    for index in range(0, len(question_ids)):\n",
    "        key_dict[question_ids[index]] = answers[index] # assign an answer to each question_id\n",
    "    return key_dict\n",
    "\n",
    "# Function for df.apply to compare whether a row is the same as another on some subset of the df \n",
    "# Used to compare key_dict to a student row\n",
    "def compare_row_to_dict(row, dict_to_compare):\n",
    "    return row.eq(pd.Series(dict_to_compare))\n",
    "\n",
    "# Creates 0/1 df by matching key_dict to each student's response\n",
    "def true_false_df(df, key_dict):\n",
    "    tf_df = df.apply(compare_row_to_dict, axis=1, args=(key_dict,)) # axis=1 for row matching, additional args for the key\n",
    "    return tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions to manipulate raw dfs to create dfs by exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove students & questions with 100% scores and 0% scores as these cause issues with ability and difficulty estimates\n",
    "def remove_issue_scores(df):\n",
    "    temp_df = df.copy()\n",
    "    len_of_key = len(df.keys()) # Used to check for total number of questions for exam\n",
    "    temp_df['score']=temp_df.sum(axis=1) # Column with student's total number of correct responses\n",
    "\n",
    "    if len_of_key in temp_df['score'].unique(): # If any students got every single question correct\n",
    "        drop_list_100s=temp_df[temp_df['score'] == len_of_key].index.to_list() # List of indicies for students who scored 100%\n",
    "        temp_df.drop(drop_list_100s, inplace=True) # Drop rows by indicies\n",
    "        #print(f'{len(drop_list_100s)} 100% scores were dropped.') \n",
    "    if 0 in temp_df['score'].unique(): # If any students got no question correct\n",
    "        drop_list_0s=temp_df[temp_df['score'] == 0].index.to_list() \n",
    "        temp_df.drop(drop_list_0s, inplace=True) # List of indicies for students who scored 0%\n",
    "        #print(f'{len(drop_list_0s)} 0% scores were dropped.')\n",
    "    temp_df.drop(['score'], axis=1, inplace=True) # Drop rows by indicies\n",
    "\n",
    "    question_score_series=temp_df.mean(axis=0) # Series to check for 100% and 0% scores by question (axis=0 for columns)\n",
    "    \n",
    "    question_0_score_series=question_score_series.where(question_score_series == 0).dropna() # Returns reduced series where average response to question was 0 (which implies all students got it wrong or omitted)\n",
    "    if len(question_0_score_series) > 0:\n",
    "        q0_index_list=question_0_score_series.index.to_list() # List of indicies for questions with scores 0%\n",
    "        temp_df.drop(q0_index_list, axis=1, inplace=True) # Drop columns by indicies, but column names are now rows of the Series\n",
    "\n",
    "    question_100_score_series=question_score_series.where(question_score_series == 1).dropna() # Returns reduced series where average response to question was 1 (which implies all students got it right or omitted)\n",
    "    if len(question_100_score_series) > 0:\n",
    "        q100_index_list=question_100_score_series.index.to_list() # List of indicies for questions with scores 100%\n",
    "        temp_df.drop(q100_index_list, axis=1, inplace=True) # Drop columns by indicies, but column names are now rows of the Series\n",
    "\n",
    "    return temp_df\n",
    "\n",
    "# Defines list of all exam numbers [character 1] and exam forms [character 2]\n",
    "def collect_all_exam_numbers_and_forms(df):\n",
    "    all_question_ids=df['question_id'].tolist()\n",
    "    all_exam_numbers_and_forms=list(set([x[0:2] for x in all_question_ids])) # list comprehension for first two characters of question_id list, then cast to set for uniqueness, then returned to list\n",
    "    return all_exam_numbers_and_forms\n",
    "\n",
    "def create_true_false_for_all_exams(full_df, key_df, all_exam_numbers_and_forms):\n",
    "    list_of_tf_dfs=[] # initialize future list of true/false dfs for Rasch model\n",
    "    for exam_num_and_form in all_exam_numbers_and_forms:\n",
    "        temp_exam_df=exam_num_ver_df(exam_num_and_form, full_df) # temp df based on exam number and form\n",
    "        temp_exam_responses_df=questions_to_columns(temp_exam_df) # convert to students by row with question_ids as columns\n",
    "        temp_exam_answer_key=create_num_ver_key_dict(exam_num_and_form, key_df) # create answer key for scoring student responses\n",
    "        temp_exam_tf_df=true_false_df(temp_exam_responses_df, temp_exam_answer_key).astype(int) # score student responses with answer key\n",
    "        list_of_tf_dfs.append({'exam_num_and_form': exam_num_and_form, 'true_false_df': temp_exam_tf_df}) # return 0/1 df by name\n",
    "    return list_of_tf_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions for Rasch calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for df.apply() to create ability estimate by row\n",
    "def ability_estimate(row):\n",
    "    return math.log(row['avg_student_score'] / (1 - row['avg_student_score']))\n",
    "\n",
    "# Function for df.apply() to create difficulty estimate by column\n",
    "def difficulty_estimate(col):\n",
    "    return math.log((1 - col['avg_question_score'] ) / col['avg_question_score'])\n",
    "\n",
    "# Calculuate ability and difficulty estimates based on 0/1 df\n",
    "def approximate_ability_and_difficulty(df):\n",
    "    temp_df=df.copy() # creates a copy just in case \n",
    "    temp_df['avg_student_score']=temp_df.mean(axis=1) # mean by row, where mean of 0/1s is overall score on exam\n",
    "    theta_s=temp_df.apply(ability_estimate, axis=1).tolist() # df.apply returns a single value based on the row, which would be a Series that we convert to list\n",
    "    temp_df.drop(['avg_student_score'], axis=1, inplace=True) # drop the 'avg_student_score' so a difficulty estimate is not made based on the column\n",
    "\n",
    "    temp_df.loc['avg_question_score']=temp_df.mean(axis=0) # mean by column, where mean of 0/1s is overall score on question\n",
    "    beta_i_non_normal=temp_df.apply(difficulty_estimate, axis=0) # df.apply returns a single value based on the column, returns as Series\n",
    "    # temp_df.drop(['avg_question_score'], inplace=True) # depreciated since df no longer returned\n",
    "    avg_beta_i=beta_i_non_normal.mean() # calculates average beta_i (difficulty estimate for item i) to normalize\n",
    "    beta_i=beta_i_non_normal - avg_beta_i # normalizes difficulty estimates\n",
    "    beta_i_keys=beta_i.keys().tolist() # Question names for dict keys, needed for calc_expected_values function\n",
    "    return {'beta_i_keys': beta_i_keys, 'beta_i': beta_i, 'theta_s': theta_s} # returns ability and difficulty estimate with question_id keys for future columns\n",
    "\n",
    "# If error is too large, adjust beta_i and theta_s by sum of error / sum of variance \n",
    "def iterate_variable_estimates(variable_estimates_dict, variance_df, residuals_df):\n",
    "    beta_i=list(variable_estimates_dict['beta_i'])\n",
    "    theta_s=list(variable_estimates_dict['theta_s'])\n",
    "    beta_i_keys=list(variable_estimates_dict['beta_i_keys']) # Not used in this function but is returned for new variable_estimates_dict\n",
    "\n",
    "    new_beta_i=[]\n",
    "    for beta_index in range(0, len(beta_i)):\n",
    "        col_names=variance_df.columns.tolist() # Track column names by variance_df (which matches residuals_df) in case questions were thrown out due to 100% or 0%\n",
    "        temp_key=col_names[beta_index] # question_i associated to beta_i\n",
    "        residual_col_sum=residuals_df[temp_key].sum() # sum of residuals by column (i)\n",
    "        variance_col_sum=variance_df[temp_key].sum() # sum of variance by column (i)\n",
    "        temp_new_beta = beta_i[beta_index] - (residual_col_sum / variance_col_sum) # new estimate is old beta_i - sum of error by column / variance by column\n",
    "        new_beta_i.append(temp_new_beta) \n",
    "\n",
    "    new_theta_s=[]\n",
    "    for theta_index in range(0, len(theta_s)):\n",
    "        index_names=variance_df.index.values.tolist() # Uses variance_df indicies in case students were thrown out due to 100% or 0%\n",
    "        temp_index=index_names[theta_index] # student_s associated with theta_s\n",
    "        residual_row_sum=residuals_df.loc[temp_index].sum() # sum of residuals by student (s)\n",
    "        variance_row_sum=variance_df.loc[temp_index].sum() # sum of variance by student (s)\n",
    "        new_theta = theta_s[theta_index] + (residual_row_sum / variance_row_sum) # new estimate is old theta_s - sum of error by row / variance by row\n",
    "        new_theta_s.append(new_theta)\n",
    "\n",
    "    beta_mean=statistics.fmean(new_beta_i) # convert all values to float-type then compute mean, faster than .mean\n",
    "    new_beta_i=[x-beta_mean for x in new_beta_i] # normalizes difficulty estimates, as before\n",
    "\n",
    "    return {'beta_i': new_beta_i, 'theta_s': new_theta_s, 'beta_i_keys': beta_i_keys}\n",
    "\n",
    "# Expected values are the probability of student s answering question i correctly given a student's ability score theta_i and the item's difficulty beta_i\n",
    "# Matched against student responses (1/0) on exam\n",
    "def calc_expected_values(variable_estimates_dict):\n",
    "    beta_i_keys=list(variable_estimates_dict['beta_i_keys'])\n",
    "    beta_i=list(variable_estimates_dict['beta_i'])\n",
    "    theta_s=list(variable_estimates_dict['theta_s'])\n",
    "\n",
    "    list_of_ev_dicts=[]\n",
    "    # Iterrate by rows, then by columns to create each entry\n",
    "    for theta_index in range(0, len(theta_s)):\n",
    "        temp_ev_dict={} # initalize new row for theta_index\n",
    "        for beta_index in range(0, len(beta_i)): # iterate by column to create row dict\n",
    "            exp_vars=math.exp(theta_s[theta_index] - beta_i[beta_index]) # Rasch model of 1PL with alpha=1\n",
    "            temp_ev_dict[beta_i_keys[beta_index]] = exp_vars / (1 + exp_vars) # probability of student s answering question i correctly given a student's ability score theta_i and the item's difficulty beta_i\n",
    "        list_of_ev_dicts.append(temp_ev_dict) \n",
    "\n",
    "    ev_df=pd.DataFrame(list_of_ev_dicts)\n",
    "    return ev_df\n",
    "\n",
    "# Calculates variance dataframe for itereate_variable_estimates function\n",
    "def calc_est_var(df):\n",
    "    return df.apply(lambda x: x*(1-x)) # variance of binomial distribution is n*p*(1-p), where n=1 for variance of a single (_i)(_s) entry\n",
    "\n",
    "# function to find sum of squares by row (equal to sum of squares by column)\n",
    "def calc_sum_sqr_residuals(df):\n",
    "    temp_series_sum = df.sum(axis=1) # sum by row (axis=1)\n",
    "    temp_series_sum = temp_series_sum.pow(2) # .pow(n) raises each Series entry to the nth power\n",
    "    sum_of_sqrs = temp_series_sum.sum() # sum the squares of each Series entry\n",
    "    return sum_of_sqrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rasch calculation definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rasch_model(base_df):\n",
    "    student_ids=base_df.index.tolist() # Save student_ids to apply at end\n",
    "    first_iteration=1 # first iteration will approximate ability and difficulty, all others will iterate the variables\n",
    "    sum_sqr_res=1 # forces while to fail on first iteration and is calculated later\n",
    "    iteration_num=0 # only used for testing\n",
    "    while sum_sqr_res > 0.0001: # while sum of errors is \"large\"\n",
    "        if first_iteration==1:\n",
    "            iteration_num=1 # only used for testing\n",
    "            first_iteration=0 # forces future iterations to iterate on future ability and difficulty estimates\n",
    "            variable_estimates_dict=approximate_ability_and_difficulty(base_df) # initial ability estimates by student and difficulty estimates by item\n",
    "        else:\n",
    "            iteration_num+=1\n",
    "            variable_estimates_dict=iterate_variable_estimates(variable_estimates_dict, est_var_ex_vals_df, residuals_df) # modifies ability and difficulty estimates by giving more weight to ability and less to difficulty\n",
    "        expected_values_df=calc_expected_values(variable_estimates_dict) # probability a student s answers question i correctly\n",
    "        est_var_ex_vals_df=calc_est_var(expected_values_df) # variance of expected values as 1*p*(1-p)\n",
    "        base_df.index=expected_values_df.index # ensure indicies between base_df and expected_values_df are equal for subtraction of dfs\n",
    "        residuals_df=base_df-expected_values_df # difference between actual response scores and probability based on student ability and item difficulty\n",
    "        sum_sqr_res=calc_sum_sqr_residuals(residuals_df) # sum of errors between actual response scores and probability\n",
    "\n",
    "    fit_df=residuals_df.pow(2)/est_var_ex_vals_df # final normalized error for each expected value\n",
    "    fit_df.index=student_ids # applies original index of base_df\n",
    "\n",
    "    var_estimates_students=pd.Series(variable_estimates_dict['theta_s'], index=student_ids) # variance for ability estimates by student\n",
    "    var_estimates_students.name='var_estimates_students'\n",
    "    var_estimates_items=pd.Series(variable_estimates_dict['beta_i'], index=variable_estimates_dict['beta_i_keys']) # variance for difficulty estimates by item\n",
    "    var_estimates_items.name='var_estimates_items'\n",
    "\n",
    "    # Outfit (Outlier-Sensitivity fit) Unweighted Fit Mean Square\n",
    "    # Sensitive to \"outer\" outliers (difficulty and ability far apart)\n",
    "    outfit_students=fit_df.mean(axis=1) \n",
    "    outfit_students.index=student_ids\n",
    "    outfit_students.name='outfit_students'\n",
    "\n",
    "    outfit_items=fit_df.mean(axis=0)\n",
    "    outfit_items.name='outfit_items'\n",
    "\n",
    "    # Infit (Inlier-Sensitivity fit) Weighted Fit Mean Square\n",
    "    # Sensitive to \"inner\" outliers (unexpected performance on items at student difficulty level)\n",
    "    infit_students=residuals_df.pow(2).sum(axis=1)/est_var_ex_vals_df.sum(axis=1) # Weighted by variance\n",
    "    infit_students.index=student_ids\n",
    "    infit_students.name='infit_students'\n",
    "\n",
    "    infit_items=residuals_df.pow(2).sum(axis=0)/est_var_ex_vals_df.sum(axis=0) # Weighted by variance\n",
    "    infit_items.name='infit_items'\n",
    "\n",
    "    rasch_dict={'fit_df': fit_df, \n",
    "            'var_estimates_students': var_estimates_students, # returns as Series\n",
    "            'var_estimates_items': var_estimates_items, # returns as Series\n",
    "            'outfit_students': outfit_students, # returns as Series\n",
    "            'outfit_items': outfit_items, # returns as Series\n",
    "            'infit_students': infit_students, # returns as Series\n",
    "            'infit_items': infit_items # returns as Series\n",
    "            }\n",
    "    return rasch_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Rasch for all exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exam_numbers_and_forms=collect_all_exam_numbers_and_forms(raw_df) # creates list of exam numbers and forms from df\n",
    "list_of_tf_dfs=create_true_false_for_all_exams(raw_df, key_df, all_exam_numbers_and_forms) # creates list of dicts with true/false dataframes based on key_df provided and the exam number+form\n",
    "\n",
    "list_of_rasch_dicts=[]\n",
    "for exam_dict in list_of_tf_dfs:\n",
    "    no_error_exam_df=remove_issue_scores(exam_dict['true_false_df']) # removes 0% and 100% from student rows and question columns\n",
    "    rasch_dict=build_rasch_model(no_error_exam_df) # iterates through until error is effectively 0\n",
    "    rasch_dict['exam_num_and_form']=exam_dict['exam_num_and_form'] # extract exam number and form from \n",
    "    rasch_dict['true_false_df']=exam_dict['true_false_df'] # save originally graded dataframe based on student responses\n",
    "    list_of_rasch_dicts.append(rasch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins list of Series on their index to create DataFrame\n",
    "def join_series_from_list_on_index(list_of_series):\n",
    "    list_of_dfs=[pd.DataFrame(series, columns=[series.name]) for series in list_of_series] # convert Series to DataFrame to use df.join()\n",
    "    if len(list_of_dfs) == 0:\n",
    "        joined_dfs=pd.DataFrame() # This shouldn't happen but it put just in case\n",
    "    elif len(list_of_dfs) == 1:\n",
    "        joined_dfs=list_of_dfs[0] # Single Series was already converted to DataFrame, returning that DataFrame\n",
    "    else:\n",
    "        first_df=list_of_dfs.pop(0) # Remove the first df from the list as the base df to join on\n",
    "        joined_dfs=first_df.join(list_of_dfs, how='inner') # join the rest of the dfs to the first on the shared indicies \n",
    "    return joined_dfs\n",
    "\n",
    "# Create a student and item df based on rasch statistics by column\n",
    "def build_rasch_dfs(list_of_rasch_dicts):\n",
    "    list_of_student_dfs=[] # initalize list to be converted to df\n",
    "    list_of_item_dfs=[] # initalize list to be converted to df\n",
    "    for rasch_dict in list_of_rasch_dicts:\n",
    "        student_statistics=['var_estimates_students', 'outfit_students', 'infit_students']\n",
    "        student_partial_series_list=[rasch_dict[s_key] for s_key in student_statistics] # list of rasch Series statistics on students\n",
    "        temp_student_df=join_series_from_list_on_index(student_partial_series_list) # Joins Series into single df joined on shared indicies\n",
    "\n",
    "        temp_standard_error=math.sqrt(2/len(temp_student_df)) # standard error to determine outliers as 2 more than fit value of 1\n",
    "\n",
    "        temp_student_df['is_outfit_outlier']=(temp_student_df['outfit_students'] > 1+2*temp_standard_error) # create 0/1 column to easily sort in Excel\n",
    "        temp_student_df['is_infit_outlier']=(temp_student_df['infit_students'] > 1+2*temp_standard_error) # create 0/1 column to easily sort in Excel\n",
    "        temp_student_df=temp_student_df.astype({'is_outfit_outlier': 'int', 'is_infit_outlier': 'int'}) # cast from True/False to 1/0\n",
    "\n",
    "        list_of_student_dfs.append(temp_student_df) # Add student statistics for specific exam and version to list to be concat later\n",
    "\n",
    "        item_statistics=['var_estimates_items', 'outfit_items', 'infit_items'] \n",
    "        item_partial_series_list=[rasch_dict[i_key] for i_key in item_statistics] # list of rasch Series statistics on items\n",
    "        temp_item_df=join_series_from_list_on_index(item_partial_series_list) # Joins Series into single df joined on shared indicies\n",
    "\n",
    "\n",
    "        temp_item_df['is_outfit_outlier']=(temp_item_df['outfit_items'] > 1+2*temp_standard_error) # create 0/1 column to easily sort in Excel\n",
    "        temp_item_df['is_infit_outlier']=(temp_item_df['infit_items'] > 1+2*temp_standard_error) # create 0/1 column to easily sort in Excel\n",
    "        temp_item_df=temp_item_df.astype({'is_outfit_outlier': 'int', 'is_infit_outlier': 'int'}) # cast from True/False to 1/0\n",
    "\n",
    "        list_of_item_dfs.append(temp_item_df) # Add item statistics for specific exam and version to list to be concat later\n",
    "\n",
    "    rasch_students_df=pd.concat(list_of_student_dfs) # Create single students df for all exams analyzed\n",
    "    rasch_students_df.index.name='student_id'\n",
    "\n",
    "    rasch_items_df=pd.concat(list_of_item_dfs) # Create single items df for all exams analyzed\n",
    "    rasch_items_df.index.name='question_id'\n",
    "    \n",
    "    return [rasch_students_df, rasch_items_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasch_student_results_df, rasch_item_results_df = build_rasch_dfs(list_of_rasch_dicts) # create student_df and item_df with rasch results to be exported to Excel\n",
    "\n",
    "base_dir=os.getcwd().replace('Item Response Theory Analysis', '') # getcwd() gets current location, then replace removes the folder this analysis is in from the path\n",
    "database_file_path=f\"{base_dir}/question_database_schema.xlsx\"\n",
    "with pd.ExcelWriter(database_file_path, engine=\"openpyxl\", mode='a', if_sheet_exists='replace') as writer: # opens as writer to save new sheets without deleting other sheets already saved\n",
    "    rasch_student_results_df.to_excel(writer, sheet_name='rasch_students')\n",
    "    rasch_item_results_df.to_excel(writer, sheet_name='rasch_questions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
